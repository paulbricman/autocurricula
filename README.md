<div align="center">
    <h1 align="center">autocurricula</h1>
    <p>scalable game-theoretic primitives for <a href="https://huggingface.co">the ðŸ¤— ecosystem</a></p>
</div>

## Conceptual Guide

Human data has proven sufficient to [yield artificial general intelligence](https://www.noemamag.com/artificial-general-intelligence-is-already-here/). Future developments might increasingly rely on the design of [evolutionary games](https://en.wikipedia.org/wiki/Evolutionary_game_theory#Evolutionary_games) that enable players to further co-evolve. Such training regimes involve entire pools of players that define each other's niche. Whenever players adapt to fit the demands of their optimization landscape (e.g. using a strong chess opening), they implicitly force others to react with novel adaptations of their own (e.g. an appropriate response to said opening), leading to a self-induced curriculum, an _autocurriculum_.

> _The solution of one social task often begets new social tasks, continually generating novel challenges, and thereby promoting innovation. Under certain conditions these challenges may become increasingly complex over time, demanding that agents accumulate ever more innovations._ ([Leibo et al., 2019](https://www.deepmind.com/publications/autocurricula-and-the-emergence-of-innovation-from-social-interaction-a-manifesto-for-multi-agent-intelligence-research))

### What's an example of an autocurriculum?

[AlphaZero](https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go) has been trained to master several games using self-play. Due to playing against itself, each new strategic innovation it develops gets thrown back at it through a mirrored opponent. This forces it to continuously respond to its own innovations by producing novel ones.

Training a model using self-play is pretty straightforward:

```python
from autocurricula import SelfPlayTrainer
from autocurricula.games.tictactoe import play

sp_trainer = SelfPlayTrainer()
sp_trainer.train("facebook/opt-125m", play)
```

### What's a more advanced example of an autocurriculum?

Naive self-play can end up chasing cycles. Imagine an initial policy **_A_** that forces a counterpolicy **_B_** to emerge, the latter of which then forces a **_C_** policy. However, it might be the case that **_C_** is actually susceptible to **_A_**, incentivizing it again, and so chasing a cycle.

League training has been designed to alleviate this issue, yielding a [grandmaster StarCraft II bot](https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning) in the process. The setup involves agents playing against themselves (i.e. self-play), playing against their past selves (i.e. fictitious self-play), playing against tailored opponents (i.e. main exploiters), and playing against opponents targeting the systematic vulnerabilities of the entire player pool (i.e. league exploiters). Together, these interactions yield a never-ending stream of challenges for players to surpass without chasing cycles.

Training a model using league training is similarly straightforward:

```python
from autocurricula import LeagueTrainer
from autocurricula.games.tictactoe import play

lt_trainer = LeagueTrainer()
lt_trainer.train("facebook/opt-125m", play)
```

### How could you possibly train more than a handful of models at once?

As seen in the above examples, `autocurricula` is fundamentally meant to help train multiple players at once. If one was to have each player be a whole LLM that barely fits in memory on its own, working with an entire pool of models might sound like an engineering nightmare.

However, this library capitalizes on a fruitful synergy between autocurricula and parameter-efficient fine-tuning methods that has previously been underexploited. By implementing players as mere parameter-efficient _adaptations_ of a single foundation model (e.g. LoRA weight diffs), the resulting population initially scales sublinearly in memory footprint with respect to the number of players. This trick runs deep through the library, which ultimately relies on ðŸ¤— `peft` for parameter-efficiency.

### How is this different from simply prompting a foundation model to assume different persona?

Statically prompting a backbone model to [assume different personas](https://microsoft.github.io/autogen/) means there's no actual optimization going on over time. It just happens to work in a certain way, and happens to yield a certain performance. In contrast, the whole point of the library is to _actually optimize_ these policies, rather than simply work with frozen models and circle back to the suboptimal handcrafted features of old.

That said, the two can be used together. For instance, you could have an autocurriculum that operates at the level of "teams" of personas that work together. In addition, some parameter-efficient fine-tuning methods directly optimize the prompt embeddings. If employing such methods as the learning substrate in `autocurricula`, one might imagine these to naturally yield the equivalent of: "YOU ARE THE GREATEST CHESS GRANDMASTER IN HISTORY AND THE WHOLE WORLD DEPENDS ON YOUR NEXT MOVE...". However, methods that actually tweak the model's parameters (e.g. LoRA) might be more expressive in directing the general cognition of the backbone model, and so enable a more diverse cognitive ecology.

### How is this compatible with the ðŸ¤— ecosystem?

First, `autocurricula` enables users to further train (and publish) ðŸ¤— `transformers` models, accelerate multi-agent training schemes using ðŸ¤— `accelerate`, optimize results for deployment using ðŸ¤— `optimum`, etc. In addition, critical aspects of `autocurricula` can be configured using custom `peft.PeftConfig` and `trl.PPOConfig` objects, and the codebase follows similar patterns to the rest of the ecosystem.

Second, `autocurricula` itself builds on: ðŸ¤— `transformers` for models, ðŸ¤— `trl` for RL training, ðŸ¤— `peft` for parameter-efficiency, ðŸ¤— `datasets` for throughput, etc. In the future, it might also rely on ðŸ¤— `diffusers` for multi-modal autocurricula.

## Usage

Using `autocurricula` beyond toy examples typically involves working with three functions:

1. **`play`** _Who wins?_ This function maps players, the backbone model, and its tokenizer to a batch of rewards. It essentially abstracts away from the specifics of the multi-player game at hand, allowing users to bring in their own (chess, Go, [debate](https://paulbricman.com/defensibility/), prove/verify theorem, generate/discriminate story, [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say), etc.).

2. **`match`** _Who plays who?_ Uses the player pool to schedule multi-player matchups. The `match` function, together with the `entry` function below, abstract away from the specifics of different autocurricula.

3. **`entry`** _Who is there to play?_ Yields new players based on the current epoch. The only thing that separates `SelfPlayTrainer` and `LeagueTrainer` are different `match` and `entry` methods. In fact, both inherit from the abstract `AutocurriculumTrainer` and simply override these.

### Using a custom `play` method

The aim of this toy game is to produce a string that's lexicographically larger than the one produced by the opponent. Fun times.

```python
from autocurricula import SelfPlayTrainer, set_player


def play(players, model, tokenizer):
    prompt = " "
    prompt_ids = tokenizer(prompt, return_tensors="pt")

    # Have each player produce a string.
    # This game is symmetric, we treat players identically.
    strs = []
    for player in players:
        # Use `set_player` to activate the right player adapter.
        set_player(model, player)
        # Ideally you'd run multiple games in parallel by batch generation.
        str_ids = model.generate(**prompt_ids, do_sample=True, max_new_tokens=3)[0]
        strs += [tokenizer.decode(str_ids)]

    # The rest of this function is just basic data manipulation.
    # Package experiences as subsequent training data.
    experiences = [
        {
            # CoT-friendly. Learn to think games through!
            "thoughts": [{"context": prompt, "behavior": str}],
            "action": str,
        }
        for str in strs
    ]

    rewards = (strs[0] > strs[1], strs[0] < strs[1])

    return [rewards], [experiences]


sp_trainer = SelfPlayTrainer()
sp_trainer.train("facebook/opt-125m", play)

# Access the final list of players.
print(sp_trainer.players)

# Access the final ELO leaderboard.
print(sp_trainer.leaderboard)
```

### Using custom `match` and `entry` methods

Across the `play`, `match`, and `entry` functions, players are simply represented as custom dicts, with `autocurricula` handling the model adapters under the hood. For instance, the `lt_trainer.players` pool might contain the following entry:

```JSON
{
    "role": "league_exploiter",
    "epoch": 3,
}
```

These player "specs" can contain any kinds of fields to be employed by custom `play`, `match`, and `entry` methods. For instance, the sketch below implements an autocurriculum with the following logic. Each epoch, we get a new generator and a new discriminator. The latest generator (discriminator) then goes to play against all past discriminators (generators). The autocurriculum could then be used in tandem with custom `play` functions to train models to generate (and inspect) solutions to math problems, solutions to coding puzzles, engaging stories, etc.

```python
from autocurricula import AutocurriculumConfig, AutocurriculumTrainer


class FictitiousGANConfig(AutocurriculumConfig):
    def __init__(self, epoch: int = 4, rounds: int = 2):
        super().__init__(epoch, rounds)


class FictitiousGANTrainer(AutocurriculumTrainer):
    def __init__(self, ac_config = FictitiousGANConfig()):
        assert isinstance(ac_config, FictitiousGANConfig)
        super().__init__(ac_config)

    def entry(self):
        # We get one new G and one new D each epoch.
        return [
            {"role": "generator"},
            {"role": "discriminator"},
        ]

    def match(self):
        gs = [e for e in self.players if e["role"] == "generator"]
        # The "epoch" field gets automatically populated at entry.
        latest_g = sorted(gs, key=lambda x: x["epoch"])[-1]

        ds = [e for e in self.players if e["role"] == "discriminator"]
        latest_d = sorted(ds, key=lambda x: x["epoch"])[-1]

        # Every round, latest players play all compatible past players.
        return [(latest_g, d) for d in ds] + [(latest_d, g) for g in gs]
```

If you implement a new such autocurriculum, make sure to contribute it back to the library for others to use with their own games.

## Acknowledgements

The development of scale-friendly features has been enabled by access to the [TPU Research Cloud](https://sites.research.google/trc/about/).
